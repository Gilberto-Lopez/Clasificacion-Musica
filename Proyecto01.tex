\documentclass[spanish,11pt,letterpaper]{article}

\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[margin=1in]{geometry}
\usepackage[title]{appendix}
% \usepackage{graphicx}
\usepackage{hyperref}
% \usepackage{listings}
% \usepackage{xcolor}

\renewcommand{\vec}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\decimalpoint

\title{Sistema de clasificación automática de documentos\\
Clasificación de canciones por género en base a la letra}
\author{Hernández Chiapa David Felipe\\
López García Gilberto Isaac}
\affil{Facultad de Ciencias\\{\small Universidad Nacional Autónoma de México}}
\date{\small\today}

\begin{document}

\maketitle

\section{Introducción}

Los géneros musicales varian mucho de unos a otros, por los ritmos, los instrumentos musicales
usados, la manera en que se canta y también en lo que se canta. Las letras de las canciones, pueden
variar mucho de género a género, incluso se podréa llegar a pensar que en los géneros más
relajados, sean estos Pop, Country, etc., se usen palabras agradables como ‘love’, mientras que del
otro lado en géneros como el Heavy Metal, se usen palabras más agresivas, como ‘hate’ o ‘death’.
He aquí la motivación para generar un clasificador de letras de canciones, basado en el género de
estas. Para lograr esto se usarán algoritmos especiales para clasificación, como lo son Bayes
Ingenuo y los árboles de decisión J48. De igual manera se describirán los problemas que se
fueron suscitando a lo largo de la implementación de este proyecto, y una explicación de por qué
la clasificación de letras de canciones representa un problema complicado.

\section{Descripción del problema}

La clasificación de textos o documentos consiste en que dado un conjunto de textos disponibles,
sean estos articulos cientificos, articulos de revistas y en nuestro caso, canciones, se puedan
agrupar en cierta cantidad de grupos, temas o géneros. En nuestro caso, el problema que se busca
solucionar es el siguiente: Dado un conjunto de letras de canciones, ¿a que género pertence cada
una?

Existen varios factores que hacen que este problema sea difícil de resolver, como lo son las
palabras usadas en cada tipo de género, la cantidad de contracciones y modismos usados, inclusive 
la cantidad de palabras. Un ejemplo parecido a lo que intentamos hacer, seria tomar poemas de
distintas epocas de la humanidad y tratar de clasificarlos por movimeintos culturales, es decir, si
es del Renacimiento, del Barroco, etc., ya que cada moviento tenia su propia métrica, temas, y
forma de escribir. Este tipo de problemas llegan a ser complicados, incluso para los humanos, y
probablemente lo es mas para canciones, ya que el género de estas depende del ambiente que
generen y es posible que esto no solo dependa de la letra, si no, tambien de la música.

Sin embargo, en este artículo buscamos dar solución a este problema usando aprendizaje supervisado.
Existen diversos algoritmos que cumplen esta descripción de ser supervisados, como lo son  Naive
Bayes, algoritmos genéticos, maquinas vectoriales, etc. Aqui hacemos referencia a dos de estos,
árboles de decisión J48 y Naive Bayes.
\section{Descripción de la técnica elegida}

\subsection{Árboles de decisión para clasificación}

Los árboles de decisión para clasificación son una herramienta de aprendizaje
supervisado para clasificación de ejemplares en categorías en base a los
atributos de los ejemplares, esto es, decide a qué categoría pertenece el
ejemplar. El árbol es una función que recibe un vector de atributos (o características) que
representan una instancia de un fenómeno o situación, realiza una serie de
preguntas sobre los valores de dichos atributos y con esta información
decide a qué categoría pertenece.

\subsubsection{Espacio de hipótesis}

Pensemos primero en árboles de decisión binarios (o booleanos), es decir, deciden si un ejemplar
pertenece a una de dos categorías (Sí o No). Supongamos que las características
de los ejemplares de entrada son binarias también.

Un árbol de decisión es una
función que dado un vector de $n$ características $\vec{x} = (a_1,\ldots,a_n)$,
asigna un valor en $\{0,1\}$. Hay un total de $2^n$ posibles vectores de entrada,
por lo que existen un total de $2^{2^n}$ funciones de este tipo, sin embargo puede
haber árboles distintos que calculen la misma función, por ejemplo, cambiando el
orden de las preguntas sobre los atributos de $\vec{x}$. Así, el
espacio de hipótesis es el conjunto de árboles de decisión que calculan
las funciones binarias sobre vectores de $n$ características, que tiene cardinalidad
al menos $2^{2^n}$.

En el caso general donde los vectores de entrada tienen $n$ características, cada
característica $a_i$ toma valores en algún conjunto finito $A_i$, y el árbol
decide si el ejemplar pertenece a alguna de $k$ categorías distintas, nuestro espacio
de hipótesis es el conjunto de árboles que calculan funciones
$A_1\times\ldots\times A_n \rightarrow \{0,\ldots,k-1\}$, que son al menos
\[ k^r, r = \prod_{r=1}^n|A_r|. \]

\subsubsection{Entropía de una variable aleatoria e Información mútua}

La \textit{entropía} de una variable aleatoria $X$ es una medida de la cantidad
promedio de información que nos da $X$ cada vez que toma un valor, esto es, ocurre el evento
$X = x_s$. Un evento que ocurre con probabilidad 1 no nos da
información porque sabemos que va a ocurrir, pero si un evento con baja probabilidad
ocurre nos da más información sobre el fenómeno que modela $X$ en ese momento, entonces
la cantidad de información está relacionada con el inverso de la probabilidad
de ocurrencia.

Considera una variable aleatoria discreta $X$ que toma valores en un conjunto
finito $\{x_1,\ldots,x_k\}$ con probabilidad $p_s = \mathbb{P}[X = x_s]$. Definimos la
cantidad de información ganada al observar el evento $X = x_s$
como \[ I(x_s) := \log \left(\frac{1}{p_s}\right) = -\log(p_s).\footnote{La
base del logaritmo es arbitaria, cuando es 2 la unidad de medición son bits.} \]

La anterior definición cumple lo siguiente:
\begin{enumerate}
  \item Si $p_s=1$, $I(x_s) = 0$. Un evento que siempre ocurre no da información,
  \item $I(x_s) \geq 0$. La ocurrencia del evento $X = x_s$ da algo o nada de información,
  no hay pérdida,
  \item Si $p_i < p_j$, $I(x_i) > I(x_j)$. Eventos menos probables dan más información.
\end{enumerate}

Entonces, la entropía de $X$ se define como
\[ H_X := \mathbb{E}[I(x_s)] = \sum_{s=1}^k p_sI(x_s) = -\sum_{s=1}^k p_s\log(p_s).
\footnote{Consideramos $0\log(0)=0$.} \]

La extropía está acotada por $0 \leq H_X \leq \log(k)$. Es 0 cuando un evento
ocurre con probabilidad 1 y los demás con probabilidad 0 pues no ganamos información,
siempre ocurre lo mismo. Es $\log(k)$ cuando todos los eventos son equiprobables
y la incertidumbre es máxima.

Dadas dos variables aleatorias discretas $X,Y$, la \textit{información mútua} es
una medida de la información que $X$ y $Y$ comparten, esto es, cuánto se reduce
la incertidumbre de una cuando se observa la otra. Se define como
%\[I(X;Y) := \sum_{t=1}^l\sum_{s=1}^k p_{X,Y}(s,t)\log\left(\frac{p_{X,Y}(s,t)}{p_X(s)p_Y(t)}\right),\]
%donde $p_{X,Y}$ es la función de masa conjunta. Esto también puede escribirse como
\[I(X;Y) := H_Y-H_{Y|X} = H_X-H_{X|Y},\]
donde $H_{Y|X}$ es la entropía de $Y$ después de observar $X$.

\subsubsection{Algoritmos ID3 y J48}

El algoritmo ID3 de Ross Quinlan genera árboles de decisión buscando minimizar la
cantidad de preguntas sobre los atributos de un vector de entrada que deben hacerse
para decidir su clasificación seleccionando los atributos más importantes,
los que dan más información sobre el problema. Para determinar qué características son
las más importantes, busca maximizar información obtenida al observar una
característica.

Cuando un árbol pregunta sobre un atributo $E$, el conjunto de ejemplares $X$ con
etiquetas $Y$ es dividido en subconjuntos $X_1,\ldots,X_k$ (con sus correspondientes
eqiquetas $Y_1,\ldots,Y_k$) donde el atributo $E$ de todos los ejemplares en $X_i$
es $i$, para cada valor $i$ de $E$. Maximizar la
\textit{ganancia de información} es encontrar el atributo $E$ tal que la diferencia
en la entropía de $X$ antes y después de dividir el conjunto de ejemplares es
máxima:
\[\argmax_E IG(E),\]
donde
\[IG(E) := H_X - H_{X|E} = H_X - \sum_{i=1}^k \frac{|X_i|}{|X|}H_{X_i}\]
es la ganancia de información al preguntar por el atributo $E$. Aquí entiéndase
$H_X$ en términos de $I(y_i)$ la información que da un ejemplar de $X$ que tiene
etiqueta $y_i \in Y$ (pertenece a la categoría $i$). Como
información mútua, $IG(E)$ nos dice cuánto se reduce la incertidumbre en $X$ después
de observar a $E$.

El algoritmo ID3 construye el árbol de decisión como sigue:
\begin{enumerate}
  \item Si todos los ejemplares de $X$ pertenecen a la misma categoría $i$ crea un nodo con etiqueta $i$.
  \item Si ya no quedan atributos para preguntar crea un nodo con la etiqueta de la categoría más común en $X$.
  \item Encuentra el atributo $E$ que maximiza $IG(E)$ y crea un nuevo árbol de decisión con raíz $E$. Para cada valor $e$ que pueda tomar $E$ y generar los subconjuntos $X_e$ y para cada $e$:
  \begin{enumerate}
    \item Si $X_e = \varnothing$ añadir una rama $E=e$ al árbol con raíz $E$ y un nodo con etiqueta la categoría más común en $X$.
    \item Añadir una rama $E=e$ al árbol con raíz $E$ y subárbol el árbol resultante de ID3 sobre el conjunto $X_e$ (sin tomar en cuenta el atributo $E$).
  \end{enumerate}
  \item Regresar el árbol con raíz $E$.
\end{enumerate}

El algoritmo J48 de Weka es una mejora sobre el algoritmo ID3\footnote{La versión libre del
algoritmo C4.5 de Ross Quinlan.}. Permite trabajar con atributos continuos determinando
un umbral para la separación del conjunto de ejemplares (ejemplo,
atributo numérico $r$ y umbral $t$, genera subconjuntos $X_{r\leq t},X_{r > t}$). También puede
trabajar con datos incompletos (los ignora). J48 genera un árbol de decisión de
la misma manera que ID3 pero después de crearlo procede a podarlo eliminando ramas
que no contribuyen al resultado de la clasificación de ejemplares.

\subsubsection{Ventajas y desventajas}

Los árboles de decisión son herramientas sencillas, solo realizan una serie de
preguntas sobre los atributos de los ejemplares (¿qué valor tiene?) por lo que
su funcionamiento es fácil de comprender y no son muy exigentes computacionalmente,
escpecialmente cuando toman en cuenta pocos atributos para una clasificación
(lo suficientemente) exitosa.

Por otro lado, los árboles de decisión sufren de sobreentrenamiento cuando no
fue posible encontrar patrones en los datos de entrenamiento al generarlo, obteniendo árboles
grandes y complicados que no generalizan.

\subsection{Bayes ingenuo}

\section{Descripción de la propuesta e implementación}

\subsection{Datos}

Los datos para trabajar son la letra de canciones y el género asignado a la canción,
obtenidos de la base de datos del servicio de letras musicales y traducción
Musixmatch.\footnote{\url{http://www.musixmatch.com/}.}

Usando un plan de servicio gratuito para testing sólo tenemos acceso al 30\% de
la letra de cada canción. Mediante un wrapper de Python,
pymusixmatch\footnote{\url{https://github.com/hudsonbrendon/python-musixmatch}},
realizamos llamadas a la API de Musixmatch para obtener las canciones más populares
de ciertos países (BE, CA, DE, GB, MX, NO, RU, SE, US, AU, AE, CH, UG, NZ).

Se filtraron canciones para quedarnos sólo con aquellas que tenían un género musical
asignado y letra de la canción en inglés, eliminando elementos repetidos.
Más aún, había géneros musicales con pocas canciones, por lo que se eliminaron
canciones cuyo género tuviera menos de 10 canciones en el conjunto. El conjunto final
de datos consta de 575 canciones (la letra y el género de) de una cantidad
inicial de 1504.

Durante la solicitud de los datos, se eliminaron las dos últimas líneas de la letra, que
es un mensaje de Musixmatch comunicando que el dato no es para uso comercial.
La etiqueta de cada canción es \texttt{CN} donde \texttt{N} es el id del género
asignado por Musixmatch. Durante la solicitud de los datos se generó un diccionario
que contiene el id y el nombre del género en texto de los datos finales,
puede encontrarse en el Apéndice A.

\subsection{Propuesta}

\subsubsection{Preprocesamiento}

Durante la solicitud de los datos se decidió eliminar comillas dobles (\texttt{"}),
pues su uso era escaso y delimitaban frases pequeñas.

Para procesar las letras
es necesario representar el texto (en minúsculas) como vectores de entradas numéricas usando
el modelo de Weka, \texttt{StringToWordVector}. El modelo fue creado usando como
delimitadores para el tokenizer
``\texttt{\textvisiblespace\textbackslash r\textbackslash t.,;:\char`\"}%
\texttt{'‘’`()¿?!$\cdot*+-/\{\}$[]$<>$@\textbackslash n}'', generando palabras (unigramas).
Los datos incluyen gran cantidad de contracciones, \textit{slangs} y sonidos escritos, por
lo que obviamos signos de puntuación, tipos de paréntesis y símbolos poco comunes
que llegan a aparecer. Aunque las comillas simples se usan en contracciones y
modifican el significado de las palabras decidimos eliminarlas porque los datos
no son consistentes, ``you're'' llegó a aparecer como ``you\textvisiblespace re''
o con otro tipo de comillas simples que se identifican como caracteres distintos.
También se solían usar las comillas simples en modismos ('bout
en vez de about, runnin' para running) pero no en todas las canciones (gon,gon',gonna
aparecían). Todo esto generaba ruido en los datos y aumentaba el número de
características de la representación en vectores.

También se utilizó un stemmer, el modelo \texttt{IteratedLovinsStemmer} que analiza
prefijos para identificar raíces y palabras similares%
\footnote{A diferencia de \texttt{SnowballStemmer} que simplifica
la palabra eliminando sufijos hasta encontrar la raíz. Por modismos
y contracciones el algoritmo es inapropiado en este caso, aún cuando el idioma
de las letras es el inglés.}.

Finalmente, la lista de \textit{stopwords} utilizada fue \texttt{Rainbow}, además
se eliminaron manualmente tokens que consistían de números posiblemente seguidos de letras
(100k, 7am, etc.) y tokens de dos caracteres que normalmente sobran por quitar
comillas simples (``re'' de ``you're'' por ejemplo).

\subsubsection{Clasificación}

Intentaremos clasificar los textos por género utilizando dos modelos: Bayes ingenuo
y árboles de decisión J48.

Como primera aproximación tratamos de clasificar las canciones mediante las
palabras que aparecen en su letra, una idea que surge de que la
naturaleza del género reside en el ambiente que genera la canción, por ejemplo,
esperamos que el Pop cree un ambiente más amable y alegre comparado con Heavy
Metal sombrío, pensamos que encontraremos palabras como `love' y `happiness' en Pop,
`hate' y `death' en Heavy Metal, `rave' y `party' en EDM, etcétera. Podríamos esperar
entonces que un árbol de decisión sea capaz de encontrar tales patrones en los
textos y decida correctamente el género a partir de vectores booleanos (la
palabra figura en la cacnión o no).

Un problema con esto es que subgéneros toman elementos de otros
géneros por lo que pensar
que un conjunto de palabras aparecen exclusivamente en un género es una intuición errónea,
por lo que para refinar la idea anterior podemos contar las ocurrencias de las palabras
y que estas cantidades sean las características de los vectores de entrada.

\section{Conclusiones}

\begin{appendices}
\section{Diccionario de géneros}
\begin{center}
\begin{tabular}{|c|c|c||c|c|c|}
\hline
id & Género & \# canciones & id & Género & \# canciones\\
\hline
14 & Pop & 266 & 15 & R\&B/Soul & 20 \\
17 & Dance & 76 & 21 & Rock & 36 \\
18 & Hip Hop/Rap & 46 & 6 & Country & 48 \\
20 & Alternative & 83 & & & \\
\hline
\end{tabular}
\end{center}
\end{appendices}

\begin{thebibliography}{9}
\bibitem{russellnorvig}
Russell, S. J., \& Norvig, P. (2010).
\textit{Artificial intelligence: a modern approach}.
Upper Saddle River: Prentice-Hall.

\bibitem{haykin}
Haykin, S. S. (2011).
\textit{Neural networks and learning machines}.
New Dehli: PHI Learning.

\bibitem{bell}
Bell, J. (2015).
\textit{Machine learning: hands-on for developers and technical professionals}.
Indianapolis: John Wiley \& Sons.
\end{thebibliography}

\end{document}
